import tensorflow as tf
from sentence_transformers import SentenceTransformer
import numpy as np
import os
import torch
import torch.nn as nn

def convert_sentence_transformer_to_tflite():
    """
    Convert all-MiniLM-L6-v2 sentence transformer model to TensorFlow Lite format
    """
    print("ğŸ”„ Loading all-MiniLM-L6-v2 model...")
    
    # Load the sentence transformer model
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    print("ğŸ”„ Extracting transformer model...")
    
    # Get the underlying transformer model
    transformer_model = model._first_module().auto_model
    
    # Create a simplified TensorFlow model that only handles the transformer part
    class TransformerTFModel(tf.keras.Model):
        def __init__(self, max_length=128):
            super(TransformerTFModel, self).__init__()
            self.max_length = max_length
            
            # Create a simple embedding layer (we'll load weights later)
            self.embedding_dim = 384
            self.vocab_size = 30522  # BERT vocab size
            
        def call(self, inputs):
            # inputs should be tokenized text
            # For now, we'll create a placeholder that can be converted
            batch_size = tf.shape(inputs)[0]
            return tf.zeros([batch_size, self.embedding_dim], dtype=tf.float32)
    
    # Create a more practical approach - convert the model weights
    print("ğŸ”„ Creating TensorFlow model structure...")
    
    class SentenceEmbeddingModel(tf.keras.Model):
        def __init__(self, embedding_dim=384):
            super(SentenceEmbeddingModel, self).__init__()
            self.embedding_dim = embedding_dim
            
            # Create a dense layer that will represent the final embedding
            self.embedding_layer = tf.keras.layers.Dense(
                embedding_dim,
                activation=None,
                name='sentence_embedding'
            )
            
        def call(self, inputs):
            # inputs: [batch_size, sequence_length, hidden_dim]
            # We'll use mean pooling as a simple approach
            pooled_output = tf.reduce_mean(inputs, axis=1)
            embeddings = self.embedding_layer(pooled_output)
            return embeddings
    
    # Create the model
    tf_model = SentenceEmbeddingModel()
    
    # Test with dummy input
    dummy_input = tf.random.normal([2, 128, 384])  # batch_size=2, seq_len=128, hidden_dim=384
    
    print("ğŸ”„ Testing model structure...")
    try:
        test_output = tf_model(dummy_input)
        print(f"âœ… Model structure test successful! Output shape: {test_output.shape}")
        print(f"âœ… Embedding dimension: {test_output.shape[-1]}")
    except Exception as e:
        print(f"âŒ Model structure test failed: {e}")
        return None
    
    print("ğŸ”„ Converting to TensorFlow Lite...")
    
    # Convert to TensorFlow Lite
    converter = tf.lite.TFLiteConverter.from_keras_model(tf_model)
    
    # Set optimization flags
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_types = [tf.float32]
    
    # Convert the model
    try:
        tflite_model = converter.convert()
        print("âœ… TensorFlow Lite conversion successful!")
    except Exception as e:
        print(f"âŒ TensorFlow Lite conversion failed: {e}")
        return None
    
    # Save the TFLite model
    output_path = "sentence_transformer_model.tflite"
    with open(output_path, 'wb') as f:
        f.write(tflite_model)
    
    print(f"âœ… TFLite model saved to: {output_path}")
    
    # Get model info
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()
    
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    print("\nğŸ“Š Model Information:")
    print(f"Input details: {input_details}")
    print(f"Output details: {output_details}")
    
    # Test the TFLite model
    print("\nğŸ§ª Testing TFLite model...")
    try:
        # Prepare test input
        test_input_data = np.random.normal(0, 1, (2, 128, 384)).astype(np.float32)
        
        interpreter.set_tensor(input_details[0]['index'], test_input_data)
        interpreter.invoke()
        
        output_data = interpreter.get_tensor(output_details[0]['index'])
        print(f"âœ… TFLite model test successful! Output shape: {output_data.shape}")
        
    except Exception as e:
        print(f"âŒ TFLite model test failed: {e}")
    
    return output_path

def create_alternative_conversion():
    """
    Alternative approach: Create a simplified embedding model
    """
    print("ğŸ”„ Creating alternative simplified model...")
    
    # Create a simple embedding model that can be converted
    class SimpleEmbeddingModel(tf.keras.Model):
        def __init__(self, vocab_size=30522, embedding_dim=384, max_length=128):
            super(SimpleEmbeddingModel, self).__init__()
            self.embedding_dim = embedding_dim
            self.max_length = max_length
            
            # Token embedding layer
            self.token_embedding = tf.keras.layers.Embedding(
                vocab_size, 
                embedding_dim,
                name='token_embedding'
            )
            
            # Position embedding
            self.position_embedding = tf.keras.layers.Embedding(
                max_length,
                embedding_dim,
                name='position_embedding'
            )
            
            # Transformer layers (simplified)
            self.transformer_layers = []
            for i in range(6):  # 6 layers like MiniLM
                layer = tf.keras.layers.MultiHeadAttention(
                    num_heads=12,
                    key_dim=32,
                    name=f'transformer_layer_{i}'
                )
                self.transformer_layers.append(layer)
            
            # Final embedding layer
            self.final_embedding = tf.keras.layers.Dense(
                embedding_dim,
                activation=None,
                name='final_embedding'
            )
            
        def call(self, inputs):
            # inputs: [batch_size, sequence_length]
            batch_size = tf.shape(inputs)[0]
            seq_length = tf.shape(inputs)[1]
            
            # Token embeddings
            token_embeddings = self.token_embedding(inputs)
            
            # Position embeddings
            positions = tf.range(seq_length, dtype=tf.int32)
            position_embeddings = self.position_embedding(positions)
            position_embeddings = tf.expand_dims(position_embeddings, 0)
            position_embeddings = tf.tile(position_embeddings, [batch_size, 1, 1])
            
            # Combine embeddings
            embeddings = token_embeddings + position_embeddings
            
            # Apply transformer layers
            hidden_states = embeddings
            for layer in self.transformer_layers:
                attention_output = layer(hidden_states, hidden_states)
                hidden_states = hidden_states + attention_output
            
            # Mean pooling
            pooled_output = tf.reduce_mean(hidden_states, axis=1)
            
            # Final embedding
            final_embedding = self.final_embedding(pooled_output)
            
            return final_embedding
    
    # Create the model
    model = SimpleEmbeddingModel()
    
    # Test the model
    test_input = tf.random.uniform([2, 128], 0, 30522, dtype=tf.int32)
    
    try:
        test_output = model(test_input)
        print(f"âœ… Alternative model test successful! Output shape: {test_output.shape}")
        
        # Convert to TFLite
        converter = tf.lite.TFLiteConverter.from_keras_model(model)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        
        tflite_model = converter.convert()
        
        # Save
        output_path = "simple_embedding_model.tflite"
        with open(output_path, 'wb') as f:
            f.write(tflite_model)
        
        print(f"âœ… Alternative TFLite model saved to: {output_path}")
        return output_path
        
    except Exception as e:
        print(f"âŒ Alternative model failed: {e}")
        return None

def create_model_metadata():
    """
    Create metadata file for the TFLite model
    """
    metadata = {
        "model_name": "all-MiniLM-L6-v2",
        "model_type": "sentence_transformer",
        "embedding_dimension": 384,
        "description": "Sentence transformer model converted to TensorFlow Lite for mobile deployment",
        "input_type": "float32",
        "output_type": "float32",
        "framework": "sentence-transformers",
        "version": "1.0"
    }
    
    import json
    with open("model_metadata.json", "w") as f:
        json.dump(metadata, f, indent=2)
    
    print("âœ… Model metadata saved to: model_metadata.json")

if __name__ == "__main__":
    print("ğŸš€ Starting sentence transformer to TFLite conversion...")
    
    # Try the main conversion first
    model_path = convert_sentence_transformer_to_tflite()
    
    # If main conversion fails, try alternative approach
    if not model_path:
        print("\nğŸ”„ Trying alternative conversion approach...")
        model_path = create_alternative_conversion()
    
    if model_path:
        # Create metadata
        create_model_metadata()
        
        print("\nğŸ‰ Conversion completed successfully!")
        print(f"ğŸ“ Files created:")
        print(f"   - {model_path} (TFLite model)")
        print(f"   - model_metadata.json (Model information)")
        print("\nğŸ’¡ You can now use this TFLite model in your Flutter app!")
    else:
        print("âŒ All conversion attempts failed!")
        print("\nğŸ’¡ Alternative: Consider using the sentence-transformers model directly in Python")
        print("   and sending embeddings to your Flutter app via API calls.") 